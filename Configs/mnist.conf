 %%%%%%%%%%%%%%%%%%%%%  Layers specification %%%%%%%%%%%%%%%%%%
 
 	net.layers{end+1}.properties = struct('type',net.types.input, 'sizeFm' ,[28 28],'numFm',1 );         %inputLayer
	net.layers{end+1}.properties = struct('type',net.types.conv,'numFm',12  ,'kernel',5,'pad',2);
 	net.layers{end+1}.properties = struct('type',net.types.conv,'numFm',24 ,'kernel',13);
  %	net.layers{end+1}.properties = struct('type',net.types.batchNorm);
 	net.layers{end+1}.properties = struct('type',net.types.fc,'numFm',128);
 	net.layers{end+1}.properties = struct('type',net.types.fc,'numFm',10);
  %	net.layers{end+1}.properties = struct('type',net.types.softmax);
	net.layers{end+1}.properties = struct('type',net.types.regression,'lossFunc',@CrossEnt,'costFunc',@CrossEnt_Cost);     %regression Layer
 
 %%%%%%%%%%%%%%%%%%%%%  Hyper params - training %%%%%%%%%%%%%%%%%%
 
 	net.hyperParam.trainLoopCount = 1000;		%on how many images to train before evaluating the network
 	net.hyperParam.testImageNum   = 2000;   	% after each loop, on how many images to evaluate network performance
 	net.hyperParam.ni_initial     = 0.05;		% ni to start training process
 	net.hyperParam.ni_final       = 0.025;	        % final ni to stop the training process

    net.runInfoParam.verifyBP     = 1; 
    net.runInfoParam.batchNum     = 1;
    net.hyperParam.randomizeTrainingSamples = 0;
