Number of images to test: 10000 , to train: 60000, first image size:=28  28  28, var=1576.9047142047, min=0.000000, max=255.000000
    <strong>trainLoopCount</strong><strong>    testImageNum</strong><strong>    ni_initial</strong><strong>    ni_final</strong><strong>    noImprovementTh</strong><strong>    momentum</strong><strong>    constInitWeight</strong><strong>    lambda</strong><strong>    errorMethod</strong><strong>    testOnData</strong><strong>    addBackround</strong><strong>    testOnNull</strong><strong>    augmentImage</strong><strong>    augmentParams</strong><strong>    centralizeImage</strong><strong>    cropImage</strong><strong>    flipImage</strong><strong>    useRandomPatch</strong><strong>    testNumPatches</strong><strong>    selevtivePatchVarTh</strong><strong>    testOnMiddlePatchOnly</strong><strong>    normalizeNetworkInput</strong><strong>     sizeFmInput  </strong><strong>    numFmInput</strong>
    <strong>______________</strong>    <strong>____________</strong>    <strong>__________</strong>    <strong>________</strong>    <strong>_______________</strong>    <strong>________</strong>    <strong>_______________</strong>    <strong>______</strong>    <strong>___________</strong>    <strong>__________</strong>    <strong>____________</strong>    <strong>__________</strong>    <strong>____________</strong>    <strong>_____________</strong>    <strong>_______________</strong>    <strong>_________</strong>    <strong>_________</strong>    <strong>______________</strong>    <strong>______________</strong>    <strong>___________________</strong>    <strong>_____________________</strong>    <strong>_____________________</strong>    <strong>______________</strong>    <strong>__________</strong>

    500               1000            0.05          1e-05       50                 0           NaN                0         1              0             0               0             0               [1x1 struct]     0                  0            0            0                 1                 0                      0                        1                        28    28    28    1         

    <strong>storeMaxMSENet</strong><strong>    verifyBP</strong><strong>    displayConvNet</strong><strong>    iter</strong><strong>    imagesLearned</strong><strong>    maxsucessRate</strong><strong>    noImprovementCount</strong><strong>    minMSE</strong><strong>    improvementRefMSE</strong><strong>      endSeed   </strong><strong>    datasetInfo </strong>
    <strong>______________</strong>    <strong>________</strong>    <strong>______________</strong>    <strong>____</strong>    <strong>_____________</strong>    <strong>_____________</strong>    <strong>__________________</strong>    <strong>______</strong>    <strong>_________________</strong>    <strong>____________</strong>    <strong>____________</strong>

    0                 1           0                 0       0                0                0                     Inf       Inf                  [1x1 struct]    [1x1 struct]

Layer 1: Activation=Relu, dActivation=dRelu
    <strong>type</strong><strong>    numFm</strong><strong>         Activation      </strong><strong>         dActivation     </strong><strong>      kernel   </strong><strong>        pad    </strong><strong>      stride   </strong><strong>      pooling  </strong><strong>    numFmInPrevLayer</strong><strong>    sizeFmInPrevLayer</strong><strong>    dropOut</strong><strong>    inputDim</strong><strong>         out      </strong><strong>    numWeights</strong><strong>                    indexesStride                 </strong>
    <strong>____</strong>    <strong>_____</strong>    <strong>_____________________</strong>    <strong>_____________________</strong>    <strong>___________</strong>    <strong>___________</strong>    <strong>___________</strong>    <strong>___________</strong>    <strong>________________</strong>    <strong>_________________</strong>    <strong>_______</strong>    <strong>________</strong>    <strong>______________</strong>    <strong>__________</strong>    <strong>______________________________________________</strong>

    2       7        [1x1 function_handle]    [1x1 function_handle]    5    5    5    2    2    2    2    2    4    1    1    1    1                   28    28    28       1          3           14    14     7    882           [1x14 double]    [1x14 double]    [1x7 double]

Layer 2: Activation=Relu, dActivation=dRelu
    <strong>type</strong><strong>    numFm</strong><strong>         Activation      </strong><strong>         dActivation     </strong><strong>      kernel   </strong><strong>        pad    </strong><strong>      pooling  </strong><strong>    numFmInPrevLayer</strong><strong>    sizeFmInPrevLayer</strong><strong>    dropOut</strong><strong>    inputDim</strong><strong>      stride   </strong><strong>         out      </strong><strong>    numWeights</strong><strong>                    indexesStride                 </strong>
    <strong>____</strong>    <strong>_____</strong>    <strong>_____________________</strong>    <strong>_____________________</strong>    <strong>___________</strong>    <strong>___________</strong>    <strong>___________</strong>    <strong>________________</strong>    <strong>_________________</strong>    <strong>_______</strong>    <strong>________</strong>    <strong>___________</strong>    <strong>______________</strong>    <strong>__________</strong>    <strong>______________________________________________</strong>

    2       17       [1x1 function_handle]    [1x1 function_handle]    5    5    3    1    1    0    1    1    1    7                   14    14     7       1          3           1    1    1    12    12     5    8942          [1x12 double]    [1x12 double]    [1x5 double]

Layer 3: Activation=Sigmoid, dActivation=dSigmoid
    <strong>type</strong><strong>    numFm</strong><strong>    numFmInPrevLayer</strong><strong>    sizeFmInPrevLayer</strong><strong>    dropOut</strong><strong>         Activation      </strong><strong>         dActivation     </strong><strong>    out</strong><strong>    numWeights</strong>
    <strong>____</strong>    <strong>_____</strong>    <strong>________________</strong>    <strong>_________________</strong>    <strong>_______</strong>    <strong>_____________________</strong>    <strong>_____________________</strong>    <strong>___</strong>    <strong>__________</strong>

    1       128      17                  12    12     5       1          [1x1 function_handle]    [1x1 function_handle]    1      1.5668e+06

Layer 4: Activation=Sigmoid, dActivation=dSigmoid
    <strong>type</strong><strong>    numFm</strong><strong>    numFmInPrevLayer</strong><strong>    sizeFmInPrevLayer</strong><strong>    dropOut</strong><strong>         Activation      </strong><strong>         dActivation     </strong><strong>    out</strong><strong>    numWeights</strong>
    <strong>____</strong>    <strong>_____</strong>    <strong>________________</strong>    <strong>_________________</strong>    <strong>_______</strong>    <strong>_____________________</strong>    <strong>_____________________</strong>    <strong>___</strong>    <strong>__________</strong>

    1       10       128                 1                    1          [1x1 function_handle]    [1x1 function_handle]    1      1290      

Network properties:

    <strong>numLayers</strong><strong>    numOutputs</strong><strong>    version</strong><strong>      sources   </strong><strong>    numWeights</strong><strong>      sizeInput   </strong><strong>    InputNumFm</strong>
    <strong>_________</strong>    <strong>__________</strong>    <strong>_______</strong>    <strong>____________</strong>    <strong>__________</strong>    <strong>______________</strong>    <strong>__________</strong>

    4            10            1.1        [3x1 struct]    1.578e+06     28    28    28    1         

Verifying backProp..Network is OK. Verification time=15.13
Start training iterations
Iter 1  | Imgs=500  | time=69.84 | TrainErr=0.111430 | meanGrad=0.064707 | meanWeight=0.009325 | varWeight=0.000200 | MSE=1.415085 | scesRate=74.20% | minMSE=1.415085 | maxS=74.20% | ni=0.050000 | tstTime=27.40 | totalTime=97.28 | noImpCnt=0/50
Iter 2  | Imgs=1000 | time=69.88 | TrainErr=0.074488 | meanGrad=0.068294 | meanWeight=0.011404 | varWeight=0.000363 | MSE=1.758341 | scesRate=61.20% | minMSE=1.415085 | maxS=74.20% | ni=0.050000 | tstTime=27.49 | totalTime=195.33 | noImpCnt=0/50
Iter 3  | Imgs=1500 | time=70.47 | TrainErr=0.091894 | meanGrad=0.083330 | meanWeight=0.014190 | varWeight=0.000730 | MSE=1.423188 | scesRate=74.20% | minMSE=1.415085 | maxS=74.20% | ni=0.050000 | tstTime=27.67 | totalTime=294.13 | noImpCnt=1/50
Iter 4  | Imgs=2000 | time=73.02 | TrainErr=0.095585 | meanGrad=0.070855 | meanWeight=0.017036 | varWeight=0.001325 | MSE=1.726921 | scesRate=63.60% | minMSE=1.415085 | maxS=74.20% | ni=0.050000 | tstTime=27.64 | totalTime=395.44 | noImpCnt=2/50
Iter 5  | Imgs=2500 | time=71.77 | TrainErr=0.105244 | meanGrad=0.081078 | meanWeight=0.021291 | varWeight=0.002832 | MSE=1.884591 | scesRate=59.60% | minMSE=1.415085 | maxS=74.20% | ni=0.050000 | tstTime=27.59 | totalTime=495.47 | noImpCnt=3/50
Iter 6  | Imgs=3000 | time=71.74 | TrainErr=0.143418 | meanGrad=0.032595 | meanWeight=0.023911 | varWeight=0.006491 | MSE=3.746656 | scesRate=17.80% | minMSE=1.415085 | maxS=74.20% | ni=0.050000 | tstTime=27.60 | totalTime=595.47 | noImpCnt=4/50
Iter 7  | Imgs=3500 | time=73.30 | TrainErr=0.162864 | meanGrad=0.014201 | meanWeight=0.025211 | varWeight=0.009301 | MSE=4.369133 | scesRate=17.30% | minMSE=1.415085 | maxS=74.20% | ni=0.050000 | tstTime=27.56 | totalTime=696.99 | noImpCnt=5/50
Iter 8  | Imgs=4000{Operation terminated by user during <a href="matlab:matlab.internal.language.introspective.errorDocCallback('feedForward', 'C:\Users\Hagay\Desktop\mdCNN\mdCNN\feedForward.m', 31)" style="font-weight:bold">feedForward</a> (<a href="matlab: opentoline('C:\Users\Hagay\Desktop\mdCNN\mdCNN\feedForward.m',31,0)">line 31</a>)


In <a href="matlab:matlab.internal.language.introspective.errorDocCallback('backPropegate', 'C:\Users\Hagay\Desktop\mdCNN\mdCNN\backPropegate.m', 12)" style="font-weight:bold">backPropegate</a> (<a href="matlab: opentoline('C:\Users\Hagay\Desktop\mdCNN\mdCNN\backPropegate.m',12,0)">line 12</a>)
outs = feedForward(layers, input , 0);

In <a href="matlab:matlab.internal.language.introspective.errorDocCallback('Train', 'C:\Users\Hagay\Desktop\mdCNN\Trainning\Train.m', 119)" style="font-weight:bold">Train</a> (<a href="matlab: opentoline('C:\Users\Hagay\Desktop\mdCNN\Trainning\Train.m',119,0)">line 119</a>)
         [net.layers , error, dW]= backPropegate(net.layers, image, expectedOut, net.runInfoParam.iterInfo(end).ni, net.hyperParam.momentum ,
         net.hyperParam.errorMethod ,  net.hyperParam.lambda);

In <a href="matlab:matlab.internal.language.introspective.errorDocCallback('demoMnist3D', 'C:\Users\Hagay\Desktop\mdCNN\Demo\MNIST3d\demoMnist3D.m', 13)" style="font-weight:bold">demoMnist3D</a> (<a href="matlab: opentoline('C:\Users\Hagay\Desktop\mdCNN\Demo\MNIST3d\demoMnist3D.m',13,0)">line 13</a>)
net   =  Train(MNIST3d,net, 15000);
} 
demoMnist3D
multi dimentional CNN , Hagay Garty 2016 | hagaygarty@gmail.com
Initializing network..
Initializing layer 1
Initializing layer 2
[Warning: Layer 2 input plus pad is 16  16   7 , not a power of 2. Might reduce speed] 
[> In initNetWeight
  In <a href="matlab:matlab.internal.language.introspective.errorDocCallback('CreateNet', 'C:\Users\Hagay\Desktop\mdCNN\mdCNN\CreateNet.m', 32)" style="font-weight:bold">CreateNet</a> (<a href="matlab: opentoline('C:\Users\Hagay\Desktop\mdCNN\mdCNN\CreateNet.m',32,0)">line 32</a>)
  In <a href="matlab:matlab.internal.language.introspective.errorDocCallback('demoMnist3D', 'C:\Users\Hagay\Desktop\mdCNN\Demo\MNIST3d\demoMnist3D.m', 6)" style="font-weight:bold">demoMnist3D</a> (<a href="matlab: opentoline('C:\Users\Hagay\Desktop\mdCNN\Demo\MNIST3d\demoMnist3D.m',6,0)">line 6</a>)] 
Initializing layer 3
Initializing layer 4
